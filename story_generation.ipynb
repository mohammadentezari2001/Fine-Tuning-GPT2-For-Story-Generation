{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHJxUULIA2s7"
      },
      "source": [
        "# **Fine-Tuning GPT-2 for Short Story Generation**\n",
        "\n",
        "This project demonstrates fine-tuning **GPT-2**, a pre-trained language model, to generate short stories using the **TinyStories** dataset. We'll preprocess the data, fine-tune the model with early stopping, evaluate its performance, and generate new stories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NexeKc8BtnL",
        "outputId": "2b4596cd-fbb0-491e-bc4e-e79dd7951ce5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.47.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.2.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.11)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2024.12.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "pip install transformers datasets torch accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Load the TinyStories dataset**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-O0kfvw8A2s_",
        "outputId": "c4dd06e5-d856-4926-c983-000f79b7315a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text'],\n",
            "        num_rows: 2119719\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['text'],\n",
            "        num_rows: 21990\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"roneneldan/TinyStories\")\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ulk0DNEA2tB"
      },
      "source": [
        "To reduce training time, we'll sample only **5% of the dataset** from both training and validation splits. This ensures faster experimentation while retaining representative data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4y6AXVn7A2tD",
        "outputId": "055dda52-9ccd-4032-e88e-32e3007bfa79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train size (2%): 2119\n",
            "Validation size (2%): 219\n"
          ]
        }
      ],
      "source": [
        "def sample_five_percent(dataset_split):\n",
        "    total_size = len(dataset_split)\n",
        "    five_percent_size = total_size // 1000  # 2% of the dataset\n",
        "    return dataset_split.shuffle(seed=42).select(range(five_percent_size))\n",
        "\n",
        "\n",
        "def sample_five_percent1(dataset_split):\n",
        "    total_size = len(dataset_split)\n",
        "    five_percent_size = total_size // 100  # 2% of the dataset\n",
        "    return dataset_split.shuffle(seed=42).select(range(five_percent_size))\n",
        "\n",
        "# Sample 2% from train and validation splits\n",
        "train_data = sample_five_percent(dataset['train'])\n",
        "val_data = sample_five_percent1(dataset['validation'])\n",
        "\n",
        "# Check sizes of sampled datasets\n",
        "print(f\"Train size (2%): {len(train_data)}\")\n",
        "print(f\"Validation size (2%): {len(val_data)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Tokenization**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVkaHVtKA2tE"
      },
      "source": [
        "The **GPT-2 tokenizer** is used to preprocess text data by converting it into numerical format."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We'll also ensure that the **end-of-sequence (EOS)** token is used for padding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1YWThWI8A2tF"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "feIONjlUXskI",
        "outputId": "2999b3dc-44b6-4074-ee9a-4764dd13faf3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pad token: <|endoftext|>\n",
            "Pad token ID: 50256\n"
          ]
        }
      ],
      "source": [
        "print(\"Pad token:\", tokenizer.pad_token)  # Should print \"<|endoftext|>\"\n",
        "print(\"Pad token ID:\", tokenizer.pad_token_id)  # Should print an integer ID\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VPopH4jA2tG"
      },
      "source": [
        "The dataset is tokenized into sequences of numerical tokens with **padding** and **truncation** applied to ensure all sequences are of the same length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwLiRiruA2tG",
        "outputId": "7f9bc8cd-1a8e-4e1d-abf7-2684efda8741"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'input_ids': [14967, 290, 32189, 588, 284, 711, 287, 262, 3952, 13, 1119, 766, 257, 1263, 3430, 319, 262, 2323, 13, 632, 318, 7586, 290, 890, 290, 4334, 13, 198, 198, 1, 8567, 11, 257, 3430, 2474, 5045, 1139, 13, 366, 40, 460, 10303, 340, 2474, 198, 198, 1544, 8404, 284, 10303, 262, 3430, 11, 475, 340, 318, 1165, 5802, 13, 679, 8953, 866, 290, 10532, 262, 3430, 13, 198, 198, 1, 46, 794, 2474, 339, 1139, 13, 366, 2504, 5938, 2474, 198, 198, 44, 544, 22051, 13, 1375, 318, 407, 1612, 11, 673, 655, 6834, 340, 318, 8258, 13, 198, 198, 1, 5756, 502, 1949, 2474, 673, 1139, 13, 366, 40, 460, 5236, 340, 2474, 198, 198, 3347, 11103, 510, 262, 3430, 290, 7584, 340, 319, 607, 1182, 13, 1375, 11114, 6364, 290, 7773, 13, 1375, 857, 407, 2121, 866, 13, 198, 198, 1, 22017, 2474, 5045, 1139, 13, 366, 1639, 389, 922, 379, 22486, 2474, 198, 198, 1, 10449, 345, 2474, 32189, 1139, 13, 366, 1026, 318, 1257, 2474, 198, 198, 2990, 1011, 4962, 22486, 262, 3430, 319, 511, 6665, 11, 5101, 11, 290, 7405, 13, 1119, 423, 257, 1256, 286, 1257, 351, 262, 3430, 13, 1119, 389, 3772, 290, 6613, 13, 1119, 389, 922, 2460, 13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [14967, 290, 32189, 588, 284, 711, 287, 262, 3952, 13, 1119, 766, 257, 1263, 3430, 319, 262, 2323, 13, 632, 318, 7586, 290, 890, 290, 4334, 13, 198, 198, 1, 8567, 11, 257, 3430, 2474, 5045, 1139, 13, 366, 40, 460, 10303, 340, 2474, 198, 198, 1544, 8404, 284, 10303, 262, 3430, 11, 475, 340, 318, 1165, 5802, 13, 679, 8953, 866, 290, 10532, 262, 3430, 13, 198, 198, 1, 46, 794, 2474, 339, 1139, 13, 366, 2504, 5938, 2474, 198, 198, 44, 544, 22051, 13, 1375, 318, 407, 1612, 11, 673, 655, 6834, 340, 318, 8258, 13, 198, 198, 1, 5756, 502, 1949, 2474, 673, 1139, 13, 366, 40, 460, 5236, 340, 2474, 198, 198, 3347, 11103, 510, 262, 3430, 290, 7584, 340, 319, 607, 1182, 13, 1375, 11114, 6364, 290, 7773, 13, 1375, 857, 407, 2121, 866, 13, 198, 198, 1, 22017, 2474, 5045, 1139, 13, 366, 1639, 389, 922, 379, 22486, 2474, 198, 198, 1, 10449, 345, 2474, 32189, 1139, 13, 366, 1026, 318, 1257, 2474, 198, 198, 2990, 1011, 4962, 22486, 262, 3430, 319, 511, 6665, 11, 5101, 11, 290, 7405, 13, 1119, 423, 257, 1256, 286, 1257, 351, 262, 3430, 13, 1119, 389, 3772, 290, 6613, 13, 1119, 389, 922, 2460, 13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]}\n"
          ]
        }
      ],
      "source": [
        "# Function to tokenize and prepare inputs/labels\n",
        "def tokenize_function_with_labels(examples):\n",
        "    tokenized = tokenizer(\n",
        "        examples['text'],\n",
        "        truncation=True,\n",
        "        padding=\"longest\",  # Pad to a fixed length for batches\n",
        "        max_length=512         # Set maximum length for sequences\n",
        "    )\n",
        "    # Add labels (same as input_ids for language modeling)\n",
        "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
        "    return tokenized\n",
        "\n",
        "# Tokenize train and validation datasets\n",
        "train_dataset = train_data.map(tokenize_function_with_labels, batched=True, remove_columns=[\"text\"])\n",
        "val_dataset = val_data.map(tokenize_function_with_labels, batched=True, remove_columns=[\"text\"])\n",
        "\n",
        "# Verify tokenized data structure\n",
        "print(train_dataset[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtB-vySyA2tH"
      },
      "source": [
        "We load the pre-trained **GPT-2** model and adjust the token embeddings to account for the padding token added during tokenization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Loading Model And Tokenizer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S91gx3wOA2tI",
        "outputId": "54b9fa7d-dc97-4ab6-e968-bf6daed0b280"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Embedding(50257, 768)"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import GPT2LMHeadModel\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "model.resize_token_embeddings(len(tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Test Model Before Fine-Tuning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjb7nlnrYVMK",
        "outputId": "ec83672e-c662-4e12-d9dc-dceb32be3984"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated Story 0:\n",
            "Once upon a time in a dark and feary forest near a forest clearing, young boys from a nearby village come to an open meeting. A young man with a mustache and a big red beard tells the young man about the mysterious and ominous \"Dark One.\" When the young man says his name he immediately says, \"Father.\" The young man answers in one word: \"Dark one.\"\n",
            "\n",
            "Suddenly, the darkness is not very dark and it only lasts three seconds. The man, who has nothing in common with the dark man, leaves the tree. The young man who has never been there at all begins to cry, \"Father! Father!\"\n",
            "\n",
            "And what a pity to see a father, of our age and with only one leg\n",
            "Generated Story 1:\n",
            "Once upon a time in a dark and feary forest, an unassuming humanoid fell asleep in its wake. In the early evening hours of March 18th, the monster awoke from its slumber. It was a tall, six-legged human, carrying a large, bright scarlet hand. The creature had just been seen on the battlefield, walking along the tracks of a human and a giant, but he seemed to have lost his mind (though in a way.)\n",
            "\n",
            "For about three months the \"giant\" had been waiting for the young male to come to the forest and show him his strength, but the beast eventually vanished. The creature took its hide, and when the creature took another one, the \"giant\" became aware\n",
            "Generated Story 2:\n",
            "Once upon a time in a dark and feary forest, a black dragon lurched forth from its slumber and began to destroy all that would belong to one.\n",
            "\n",
            "For centuries, this beast, which bears one of them's markings, has been called the Dragon's Dream.\n",
            "\n",
            "But one day one day, as it descended with the dragon's blood, it was taken care of and carried to Heaven, where it was kept and raised for a full lifetime.\n",
            "\n",
            "After spending eternity in heaven as a dragon, its soul returned to where it belonged and was never left behind. After that, its life did not change.\n",
            "\n",
            "It has retained its former glory and position as the divine being at the heart of the universe.\n",
            "\n",
            "Generated Story 3:\n",
            "Once upon a time in a dark and feary forest, the world of Death came.\n",
            "\n",
            "When the final battle began - the final Battle at the End of the World - the entire galaxy was plunged into darkness, and a world that had a greater potential than what the universe today had ever been.\n",
            "\n",
            "The last thing Humanity needed was for this last of its humanity to rise again. What they needed, they would be able to do.\n",
            "\n",
            "And the time to leave was upon them.\n",
            "\n",
            "(click here to order book to order. For your convenience we have a larger selection of \"Lost in Space\" chapters on our website, check out http://www.hughstamson.com/lost-narrative\n",
            "Generated Story 4:\n",
            "Once upon a time in a dark and feary forest, you will encounter another large, dangerous black bear. Don't touch it, but make sure it is safe from you. While your team has been successfully carrying out this task, your companion will start working with the bear for its survival. Now, when the bear attacks, it will attempt to attack you again. The bear's natural instincts are to stay on prey. When the beast comes near you, it will begin attacking. It's time to start treating your companions as if they were people rather than animals, because, well...it's so freaking out. If you don't stop the bear from attacking you, you may be caught in the act of attacking it. While you talk to\n",
            "Generated Story 5:\n",
            "Once upon a time in a dark and feary forest my young body was so fearful and afraid the day after that the first thing that struck me, I was thrown in the open pit beside a small cave with a lot of moss. And a huge lump stood in the middle of the cave, almost an inch in size. In it were four large, sharp, golden eyes. When I looked up, one was holding mine in its left hand and an other was holding its right. I was afraid the eyes opened into their usual eyes. I looked at the two small eyes and then looked in both of them, and I saw that one had turned around and was staring at me. I began to giggle and I was still trying to get up\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "story_generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Provide a prompt to generate a story\n",
        "prompt = \"Once upon a time in a dark and feary forest\"\n",
        "generated_story = story_generator(prompt, max_length=150, num_return_sequences=1)\n",
        "generated_story1 = story_generator(prompt, max_length=150, num_return_sequences=1)\n",
        "generated_story2 = story_generator(prompt, max_length=150, num_return_sequences=1)\n",
        "generated_story3 = story_generator(prompt, max_length=150, num_return_sequences=1)\n",
        "generated_story4 = story_generator(prompt, max_length=150, num_return_sequences=1)\n",
        "generated_story5 = story_generator(prompt, max_length=150, num_return_sequences=1)\n",
        "\n",
        "# Display the generated story\n",
        "print(\"Generated Story 0:\")\n",
        "print(generated_story[0]['generated_text'])\n",
        "print(\"Generated Story 1:\")\n",
        "print(generated_story1[0]['generated_text'])\n",
        "print(\"Generated Story 2:\")\n",
        "print(generated_story2[0]['generated_text'])\n",
        "print(\"Generated Story 3:\")\n",
        "print(generated_story3[0]['generated_text'])\n",
        "print(\"Generated Story 4:\")\n",
        "print(generated_story4[0]['generated_text'])\n",
        "print(\"Generated Story 5:\")\n",
        "print(generated_story5[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Evaluation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **-- Coherence -- Diversity -- Fluency --**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0f7Zhv1Q3sU",
        "outputId": "ffe808c7-c412-44d8-d634-2f7b8d504444"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------\n",
            "Diversity (Type-Token Ratio): 0.664\n",
            "Diversity (Entropy): 6.0376781369153365\n",
            "Coherence: 0.39965156217416126\n",
            "fluency: 17.178083419799805\n",
            "-------------------\n",
            "Diversity (Type-Token Ratio): 0.7142857142857143\n",
            "Diversity (Entropy): 6.089016793459786\n",
            "Coherence: 0.45124289989471433\n",
            "fluency: 18.006610870361328\n",
            "-------------------\n",
            "Diversity (Type-Token Ratio): 0.7166666666666667\n",
            "Diversity (Entropy): 6.15898554592641\n",
            "Coherence: 0.46206297278404235\n",
            "fluency: 16.470256805419922\n",
            "-------------------\n",
            "Diversity (Type-Token Ratio): 0.7321428571428571\n",
            "Diversity (Entropy): 6.093299846601907\n",
            "Coherence: 0.21653433237224817\n",
            "fluency: 17.7174129486084\n",
            "-------------------\n",
            "Diversity (Type-Token Ratio): 0.7741935483870968\n",
            "Diversity (Entropy): 6.393841927682315\n",
            "Coherence: 0.2862136835853259\n",
            "fluency: 15.372954368591309\n",
            "-------------------\n",
            "Diversity (Type-Token Ratio): 0.6397058823529411\n",
            "Diversity (Entropy): 6.058124489758668\n",
            "Coherence: 0.3475918446977933\n",
            "fluency: 18.52735137939453\n"
          ]
        }
      ],
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from collections import Counter\n",
        "import torch\n",
        "import math\n",
        "\n",
        "# Example text\n",
        "text = generated_story[0]['generated_text']\n",
        "text1 = generated_story1[0]['generated_text']\n",
        "text2 = generated_story2[0]['generated_text']\n",
        "text3 = generated_story3[0]['generated_text']\n",
        "text4 = generated_story4[0]['generated_text']\n",
        "text5 = generated_story5[0]['generated_text']\n",
        "\n",
        "\n",
        "# Load Sentence-BERT model for coherence\n",
        "coherence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Function to calculate Type-Token Ratio (Diversity)\n",
        "def calculate_ttr(text):\n",
        "    words = text.split()\n",
        "    unique_words = set(words)\n",
        "    ttr = len(unique_words) / len(words) if len(words) > 0 else 0\n",
        "    return ttr\n",
        "\n",
        "# Function to calculate Entropy (Diversity)\n",
        "def calculate_entropy(text):\n",
        "    words = text.split()\n",
        "    word_counts = Counter(words)\n",
        "    total_words = len(words)\n",
        "    entropy = 0.0\n",
        "    for count in word_counts.values():\n",
        "        probability = count / total_words\n",
        "        entropy -= probability * math.log(probability, 2)\n",
        "    return entropy\n",
        "\n",
        "# Function to calculate coherence\n",
        "def calculate_coherence(text):\n",
        "    sentences = [s.strip() for s in text.split('.') if s.strip()]  # Split into sentences\n",
        "    if len(sentences) < 2:\n",
        "        return 1.0  # Single sentence is trivially coherent\n",
        "    embeddings = coherence_model.encode(sentences)\n",
        "    similarities = []\n",
        "    for i in range(len(embeddings) - 1):\n",
        "        sim = util.cos_sim(embeddings[i], embeddings[i + 1])\n",
        "        similarities.append(sim.item())\n",
        "    coherence = sum(similarities) / len(similarities)\n",
        "    return coherence\n",
        "\n",
        "\n",
        "def calculate_fluency(text):\n",
        "    # Check if GPU is available\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Move model to the device\n",
        "    model.to(device)\n",
        "\n",
        "    # Tokenize the text and move inputs to the same device\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
        "\n",
        "    # Calculate perplexity\n",
        "    loss = outputs.loss\n",
        "    perplexity = torch.exp(loss)\n",
        "    return perplexity.item()\n",
        "\n",
        "# Calculate metrics\n",
        "#fluency = calculate_perplexity(text, model, tokenizer)\n",
        "ttr = calculate_ttr(text)\n",
        "entropy = calculate_entropy(text)\n",
        "coherence = calculate_coherence(text)\n",
        "fluency=calculate_fluency(text)\n",
        "\n",
        "\n",
        "\n",
        "ttr1 = calculate_ttr(text1)\n",
        "entropy1 = calculate_entropy(text1)\n",
        "coherence1 = calculate_coherence(text1)\n",
        "fluency1=calculate_fluency(text1)\n",
        "\n",
        "ttr2 = calculate_ttr(text2)\n",
        "entropy2 = calculate_entropy(text2)\n",
        "coherence2 = calculate_coherence(text2)\n",
        "fluency2=calculate_fluency(text2)\n",
        "\n",
        "ttr3 = calculate_ttr(text3)\n",
        "entropy3 = calculate_entropy(text3)\n",
        "coherence3 = calculate_coherence(text3)\n",
        "fluency3=calculate_fluency(text3)\n",
        "\n",
        "ttr4 = calculate_ttr(text4)\n",
        "entropy4 = calculate_entropy(text4)\n",
        "coherence4 = calculate_coherence(text4)\n",
        "fluency4=calculate_fluency(text4)\n",
        "\n",
        "ttr5 = calculate_ttr(text5)\n",
        "entropy5 = calculate_entropy(text5)\n",
        "coherence5 = calculate_coherence(text5)\n",
        "fluency5=calculate_fluency(text5)\n",
        "print('-------------------')\n",
        "\n",
        "# Print results\n",
        "#print(\"Fluency (Perplexity):\", fluency)\n",
        "print(\"Diversity (Type-Token Ratio):\", ttr)\n",
        "print(\"Diversity (Entropy):\", entropy)\n",
        "print(\"Coherence:\", coherence)\n",
        "print(\"fluency:\", fluency)\n",
        "print('-------------------')\n",
        "\n",
        "\n",
        "print(\"Diversity (Type-Token Ratio):\", ttr1)\n",
        "print(\"Diversity (Entropy):\", entropy1)\n",
        "print(\"Coherence:\", coherence1)\n",
        "print(\"fluency:\", fluency1)\n",
        "print('-------------------')\n",
        "\n",
        "print(\"Diversity (Type-Token Ratio):\", ttr2)\n",
        "print(\"Diversity (Entropy):\", entropy2)\n",
        "print(\"Coherence:\", coherence2)\n",
        "print(\"fluency:\", fluency2)\n",
        "print('-------------------')\n",
        "\n",
        "print(\"Diversity (Type-Token Ratio):\", ttr3)\n",
        "print(\"Diversity (Entropy):\", entropy3)\n",
        "print(\"Coherence:\", coherence3)\n",
        "print(\"fluency:\", fluency3)\n",
        "print('-------------------')\n",
        "\n",
        "print(\"Diversity (Type-Token Ratio):\", ttr4)\n",
        "print(\"Diversity (Entropy):\", entropy4)\n",
        "print(\"Coherence:\", coherence4)\n",
        "print(\"fluency:\", fluency4)\n",
        "print('-------------------')\n",
        "print(\"Diversity (Type-Token Ratio):\", ttr5)\n",
        "print(\"Diversity (Entropy):\", entropy5)\n",
        "print(\"Coherence:\", coherence5)\n",
        "print(\"fluency:\", fluency5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Training Arguments**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ye3xJf2QA2tJ"
      },
      "source": [
        "Training arguments control various aspects of training, including **learning rate**, **batch size**, **evaluation frequency**, and saving model checkpoints. The best model will be loaded at the end of training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5g2Z8n8cA2tJ",
        "outputId": "74415b70-9fbc-480f-f5d7-8d9c91d9202c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./gpt2-tinystories\",\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=100,\n",
        "    logging_steps=10, \n",
        "    learning_rate=5e-5,\n",
        "    weight_decay=0.01,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=5,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=100,\n",
        "    logging_dir=\"./logs\",\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Trainer API**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fad7EGWkA2tL"
      },
      "source": [
        "The **Trainer** API simplifies training by integrating the model, dataset, training arguments, and callbacks into one interface."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMWlbDO5A2tM",
        "outputId": "b71f938a-27ee-40ea-81ee-c978784dcd62"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-46-d0d0fad73a49>:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        }
      ],
      "source": [
        "from transformers import Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciySKczLA2tM"
      },
      "source": [
        "The model is fine-tuned on the **TinyStories** dataset, with evaluation and checkpointing performed every 500 steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 945
        },
        "id": "IVomsXsJA2tM",
        "outputId": "2b9f569f-7cfd-48a5-8324-2c94391cca32"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2650' max='2650' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2650/2650 39:33, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.956200</td>\n",
              "      <td>0.812368</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.903100</td>\n",
              "      <td>0.788236</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.898900</td>\n",
              "      <td>0.770299</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.824600</td>\n",
              "      <td>0.757766</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.822800</td>\n",
              "      <td>0.750297</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.709200</td>\n",
              "      <td>0.745406</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.735500</td>\n",
              "      <td>0.741772</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.731900</td>\n",
              "      <td>0.734896</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.745000</td>\n",
              "      <td>0.732085</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.787200</td>\n",
              "      <td>0.729332</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.684300</td>\n",
              "      <td>0.729276</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.712900</td>\n",
              "      <td>0.729391</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.675400</td>\n",
              "      <td>0.727957</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.802200</td>\n",
              "      <td>0.722578</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.745300</td>\n",
              "      <td>0.720033</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.689000</td>\n",
              "      <td>0.720689</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1700</td>\n",
              "      <td>0.648700</td>\n",
              "      <td>0.722899</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>0.688600</td>\n",
              "      <td>0.721042</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1900</td>\n",
              "      <td>0.705000</td>\n",
              "      <td>0.720016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.638000</td>\n",
              "      <td>0.719413</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2100</td>\n",
              "      <td>0.690200</td>\n",
              "      <td>0.717597</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2200</td>\n",
              "      <td>0.591400</td>\n",
              "      <td>0.720444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2300</td>\n",
              "      <td>0.641100</td>\n",
              "      <td>0.719930</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>0.669100</td>\n",
              "      <td>0.720052</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.676900</td>\n",
              "      <td>0.719532</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2600</td>\n",
              "      <td>0.640800</td>\n",
              "      <td>0.718992</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2650, training_loss=0.7341434867427035, metrics={'train_runtime': 2374.4326, 'train_samples_per_second': 4.462, 'train_steps_per_second': 1.116, 'total_flos': 2768389079040000.0, 'train_loss': 0.7341434867427035, 'epoch': 5.0})"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QR5STqaEA2tN"
      },
      "source": [
        "After training, we evaluate the model on the validation set to measure its performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "lddOTyArA2tN",
        "outputId": "1a0f3f93-fae2-4578-efce-e25ffc1b4275"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='55' max='55' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [55/55 00:09]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation Results: {'eval_loss': 0.7175973653793335, 'eval_runtime': 10.0031, 'eval_samples_per_second': 21.893, 'eval_steps_per_second': 5.498, 'epoch': 5.0}\n"
          ]
        }
      ],
      "source": [
        "results = trainer.evaluate()\n",
        "print(\"Evaluation Results:\", results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8PCuihLA2tO"
      },
      "source": [
        "Finally, we use the fine-tuned model to generate a short story based on a given prompt. The model completes the story in a coherent and creative manner."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Test Model After Fine-Tuning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iThQlTu5A2tO",
        "outputId": "6fd6cc6a-f2ed-42b7-d23b-6517a8019128"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated Story:\n",
            "Once upon a time in a dark and feary forest, there was a curious little girl named Lily. She liked to explore the forest every day.\n",
            "\n",
            "One morning, Lily saw a big box that she could play with. She was excited to see what it was! She climbed up the branches, carefully picked it out of the ground and carefully removed it from her hands. It smelled sweet and shiny.\n",
            "\n",
            "Lily's mom told her that it was a special box, so Lily was very excited. The mom explained that many things can be made with other things. Lily was so excited, she had even made a jar for the box!\n",
            "\n",
            "Her mom went outside to play with the weird things she was able to make. They\n",
            "Once upon a time in a dark and feary forest. There were two friends, a boy and a girl named Lulu. Lulu had a puppy named Max. Max lived in a warm and warm house with his mom. \n",
            "\n",
            "One night, a bad smell left the garden. Lulu and Max went outside to see if they could find the smell. They went to the ground and looked. \n",
            "\n",
            "In the grass, there was a big dog named Spot. Spot was scared and didn't like being touched. He barked and ran away. Lulu and Max were very sad. \n",
            "\n",
            "From the day on, Lulu and Max were just friends. They learned to share and be kind to others. When the\n",
            "Once upon a time in a dark and feary forest, there lived a mighty man named Mr. Smith.\n",
            "\n",
            "One day, he was walking on a path with lots of trees and bushes. Suddenly, Mr. Smith heard a voice coming from behind him!\n",
            "\n",
            "He looked closely and saw a little bird sitting on a branch. The bird said, \"Let's play hide-and-seek! We both need to rest and look after each other.\"\n",
            "\n",
            "Mr. Smith smiled and said, \"Sure, let's rest and look after each other next time.\" From that day on, his gentle nature always guided him, always reminding him to listen to his parents and nature.\n",
            "Once upon a time in a dark and feary forest there was a brave little girl called Alice. She loved to go on adventures and explore the forest.\n",
            "\n",
            "One day when she was walking down the path, she saw a little bear! She was so excited to pet it! \n",
            "\n",
            "Just then, a brave little mouse came hopping towards her. Alice was scared to go up the road with her new friend, but the mouse said, \"Alice-chan!\" and the mouse hopped behind it. \n",
            "The mouse waved goodbye and slowly flew away. Alice was delighted that she had a friend like her!\n",
            "Once upon a time in a dark and feary forest, there was a bird. The bird loved to hop and sing. Every day, the bird would hop and sing with its friends. \n",
            "\n",
            "One sunny day, the bird went for a walk and saw a bright sun! It hopped around in the sky, observing the birds and their friends. Suddenly, the bird heard a voice telling it to stop. The voice was warning it not to eat anything that was too dark and dangerous.\n",
            "\n",
            "The bird knew it had heard the voice, so it hopped away down the path and was safe. But after a while, the voice vanished, leaving behind it a scared and sad smile on its face. \n",
            "\n",
            "The bird knew it had\n",
            "Once upon a time in a dark and feary forest there lived a cute little girl. She was always afraid to go outside and explore the forest, but she never shied away. Every day she went for walks in the forest and had a lot of fun.\n",
            "\n",
            "One day, while going to investigate, she saw strange smells coming from the bushes. Ã¢â‚¬Å“Can you smell them?Ã¢â‚¬ she asked. Ã¢â‚¬Å“Yes please!Ã¢â‚¬ the plant asked. \n",
            "\n",
            "The little girl was scared too, but he nodded and they walked the forest together. It was a great day, having spent time outside and exploring.\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Create a text generation pipeline using the fine-tuned model\n",
        "story_generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Provide a prompt to generate a story\n",
        "prompt = \"Once upon a time in a dark and feary forest\"\n",
        "generated_story = story_generator(prompt, max_length=150, num_return_sequences=1)\n",
        "generated_story1 = story_generator(prompt, max_length=150, num_return_sequences=1)\n",
        "generated_story2 = story_generator(prompt, max_length=150, num_return_sequences=1)\n",
        "generated_story3 = story_generator(prompt, max_length=150, num_return_sequences=1)\n",
        "generated_story4 = story_generator(prompt, max_length=150, num_return_sequences=1)\n",
        "generated_story5 = story_generator(prompt, max_length=150, num_return_sequences=1)\n",
        "\n",
        "# Display the generated story\n",
        "print(\"Generated Story:\")\n",
        "print(generated_story[0]['generated_text'])\n",
        "print(generated_story1[0]['generated_text'])\n",
        "print(generated_story2[0]['generated_text'])\n",
        "print(generated_story3[0]['generated_text'])\n",
        "print(generated_story4[0]['generated_text'])\n",
        "print(generated_story5[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Evaluation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **-- Coherence -- Diversity -- Fluency --**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xjwc4qhaB11d",
        "outputId": "8d1fd5f0-4214-404f-bceb-7906e951188e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------\n",
            "Diversity (Type-Token Ratio): 0.6904761904761905\n",
            "Diversity (Entropy): 6.17443456241999\n",
            "Coherence: 0.3404272086918354\n",
            "fluency: 5.773502349853516\n",
            "-------------------\n",
            "Diversity (Type-Token Ratio): 0.6637931034482759\n",
            "Diversity (Entropy): 5.85592210756267\n",
            "Coherence: 0.34114815294742584\n",
            "fluency: 4.72048807144165\n",
            "-------------------\n",
            "Diversity (Type-Token Ratio): 0.7523809523809524\n",
            "Diversity (Entropy): 6.05888605182675\n",
            "Coherence: 0.2009171899408102\n",
            "fluency: 4.655630111694336\n",
            "-------------------\n",
            "Diversity (Type-Token Ratio): 0.71\n",
            "Diversity (Entropy): 5.873919005177825\n",
            "Coherence: 0.45901202857494355\n",
            "fluency: 6.44784688949585\n",
            "-------------------\n",
            "Diversity (Type-Token Ratio): 0.5887096774193549\n",
            "Diversity (Entropy): 5.75637917318541\n",
            "Coherence: 0.514707189053297\n",
            "fluency: 4.8392791748046875\n",
            "-------------------\n",
            "Diversity (Type-Token Ratio): 0.7244897959183674\n",
            "Diversity (Entropy): 5.917376207098508\n",
            "Coherence: 0.4204471686056682\n",
            "fluency: 5.4843621253967285\n"
          ]
        }
      ],
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from collections import Counter\n",
        "import torch\n",
        "import math\n",
        "\n",
        "# Example text\n",
        "text = generated_story[0]['generated_text']\n",
        "text1 = generated_story1[0]['generated_text']\n",
        "text2 = generated_story2[0]['generated_text']\n",
        "text3 = generated_story3[0]['generated_text']\n",
        "text4 = generated_story4[0]['generated_text']\n",
        "text5 = generated_story5[0]['generated_text']\n",
        "\n",
        "# Load Sentence-BERT model for coherence\n",
        "coherence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Function to calculate Type-Token Ratio (Diversity)\n",
        "def calculate_ttr(text):\n",
        "    words = text.split()\n",
        "    unique_words = set(words)\n",
        "    ttr = len(unique_words) / len(words) if len(words) > 0 else 0\n",
        "    return ttr\n",
        "\n",
        "# Function to calculate Entropy (Diversity)\n",
        "def calculate_entropy(text):\n",
        "    words = text.split()\n",
        "    word_counts = Counter(words)\n",
        "    total_words = len(words)\n",
        "    entropy = 0.0\n",
        "    for count in word_counts.values():\n",
        "        probability = count / total_words\n",
        "        entropy -= probability * math.log(probability, 2)\n",
        "    return entropy\n",
        "\n",
        "# Function to calculate coherence\n",
        "def calculate_coherence(text):\n",
        "    sentences = [s.strip() for s in text.split('.') if s.strip()]  # Split into sentences\n",
        "    if len(sentences) < 2:\n",
        "        return 1.0  # Single sentence is trivially coherent\n",
        "    embeddings = coherence_model.encode(sentences)\n",
        "    similarities = []\n",
        "    for i in range(len(embeddings) - 1):\n",
        "        sim = util.cos_sim(embeddings[i], embeddings[i + 1])\n",
        "        similarities.append(sim.item())\n",
        "    coherence = sum(similarities) / len(similarities)\n",
        "    return coherence\n",
        "\n",
        "\n",
        "def calculate_fluency(text):\n",
        "    # Check if GPU is available\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Move model to the device\n",
        "    model.to(device)\n",
        "\n",
        "    # Tokenize the text and move inputs to the same device\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
        "\n",
        "    # Calculate perplexity\n",
        "    loss = outputs.loss\n",
        "    perplexity = torch.exp(loss)\n",
        "    return perplexity.item()\n",
        "\n",
        "# Calculate metrics\n",
        "#fluency = calculate_perplexity(text, model, tokenizer)\n",
        "ttr = calculate_ttr(text)\n",
        "entropy = calculate_entropy(text)\n",
        "coherence = calculate_coherence(text)\n",
        "fluency=calculate_fluency(text)\n",
        "\n",
        "\n",
        "\n",
        "ttr1 = calculate_ttr(text1)\n",
        "entropy1 = calculate_entropy(text1)\n",
        "coherence1 = calculate_coherence(text1)\n",
        "fluency1=calculate_fluency(text1)\n",
        "\n",
        "ttr2 = calculate_ttr(text2)\n",
        "entropy2 = calculate_entropy(text2)\n",
        "coherence2 = calculate_coherence(text2)\n",
        "fluency2=calculate_fluency(text2)\n",
        "\n",
        "ttr3 = calculate_ttr(text3)\n",
        "entropy3 = calculate_entropy(text3)\n",
        "coherence3 = calculate_coherence(text3)\n",
        "fluency3=calculate_fluency(text3)\n",
        "\n",
        "ttr4 = calculate_ttr(text4)\n",
        "entropy4 = calculate_entropy(text4)\n",
        "coherence4 = calculate_coherence(text4)\n",
        "fluency4=calculate_fluency(text4)\n",
        "\n",
        "ttr5 = calculate_ttr(text5)\n",
        "entropy5 = calculate_entropy(text5)\n",
        "coherence5 = calculate_coherence(text5)\n",
        "fluency5=calculate_fluency(text5)\n",
        "print('-------------------')\n",
        "\n",
        "# Print results\n",
        "#print(\"Fluency (Perplexity):\", fluency)\n",
        "print(\"Diversity (Type-Token Ratio):\", ttr)\n",
        "print(\"Diversity (Entropy):\", entropy)\n",
        "print(\"Coherence:\", coherence)\n",
        "print(\"fluency:\", fluency)\n",
        "print('-------------------')\n",
        "\n",
        "\n",
        "print(\"Diversity (Type-Token Ratio):\", ttr1)\n",
        "print(\"Diversity (Entropy):\", entropy1)\n",
        "print(\"Coherence:\", coherence1)\n",
        "print(\"fluency:\", fluency1)\n",
        "print('-------------------')\n",
        "\n",
        "print(\"Diversity (Type-Token Ratio):\", ttr2)\n",
        "print(\"Diversity (Entropy):\", entropy2)\n",
        "print(\"Coherence:\", coherence2)\n",
        "print(\"fluency:\", fluency2)\n",
        "print('-------------------')\n",
        "\n",
        "print(\"Diversity (Type-Token Ratio):\", ttr3)\n",
        "print(\"Diversity (Entropy):\", entropy3)\n",
        "print(\"Coherence:\", coherence3)\n",
        "print(\"fluency:\", fluency3)\n",
        "print('-------------------')\n",
        "\n",
        "print(\"Diversity (Type-Token Ratio):\", ttr4)\n",
        "print(\"Diversity (Entropy):\", entropy4)\n",
        "print(\"Coherence:\", coherence4)\n",
        "print(\"fluency:\", fluency4)\n",
        "print('-------------------')\n",
        "print(\"Diversity (Type-Token Ratio):\", ttr5)\n",
        "print(\"Diversity (Entropy):\", entropy5)\n",
        "print(\"Coherence:\", coherence5)\n",
        "print(\"fluency:\", fluency5)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
